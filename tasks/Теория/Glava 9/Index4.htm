<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title></title>
<meta http-equiv="Content-Type" content="text/html;charset=windows-1251">
</head>
<body TEXT="#000000" BGCOLOR="#E7E3E7" LINK="#004080" VLINK="#004080" olink="#008080" Background="">
<table BORDER=0   COLS=3 WIDTH="16%" >
  <tr> 
    <td><font face="Arial, Helvetica, sans-serif"><a href="Index3.htm"><img SRC="Back.gif"  BORDER=0 ></a></font></td>
    <td WIDTH="10%"><font face="Arial, Helvetica, sans-serif"><a href="../index.html"><img SRC="Menu.gif" BORDER=0 ></a></font></td>
    <td ALIGN=RIGHT><font face="Arial, Helvetica, sans-serif"><a href="Index5.htm"><img SRC="For.gif" BORDER=0 ></a></font></td>
  </tr>
</table>
<p>&nbsp;</p>
<p align="center"><font face="Arial, Helvetica, sans-serif" size="3"><font size="4">9.2.1. 
  Условная вероятность</font><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Условная вероятность 
  события <i>d </i>при данном <i>s </i>— это вероятность того, что событие <i>d 
  </i>наступит при условии, что наступило событие <i>s. </i>Например, вероятность 
  того, что пациент действительно страдает заболеванием d, если у него (или у 
  нее) обнаружен только симптом <i>s.</i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В традиционной 
  теории вероятностей для вычисления условной вероятности события <i>d </i>при 
  данном <i>s </i>используется следующая формула:<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(d|s)=(d^</font></i> 
  <font face="Verdana, Arial, Helvetica, sans-serif"><i>s)/P(S)</i></font><i> 
  </i><b>(9.1)</b><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Как видно, 
  условная вероятность определяется в терминах совместимости событий. Она представляет 
  собой отношение вероятности совпадения событий <i>d </i>и <i>s </i>к вероятности 
  появления события <i>s. </i>Из формулы (9.1) следует, что<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(d^s)=P(d|s)P(d).</font></i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Если разделить 
  обе части на <i>P(s) </i>и подставить в правую часть (9.1), то получим <i>правило 
  Байеса </i>в простейшем виде:</font></p>
<p align="left"><font face="Arial, Helvetica, sans-serif" size="3"> <i><font face="Verdana, Arial, Helvetica, sans-serif">P(d|s)=(s|d)P(d)/P(S)</font></i> 
  <b>(9.2)</b><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Это правило, 
  которое иногда называют <i>инверсной формулой для условной вероятности, </i>позволяет 
  определить вероятность <i>P(d | s) </i>появления события <i>d </i>при условии, 
  что произошло событие <i>s </i>через известную условную вероятность <i>P(s | 
  d). </i>В полученном выражении <i>P(d) </i>— априорная вероятность наступления 
  события <i>d, </i>a <i>P(d | s) </i>— апостериорная вероятность, т.е. вероятность 
  того, что событие <i>d </i>произойдет, если известно, что событие <i>s </i>свершилось.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Для систем, 
  основанных на знаниях, формула (9.2) гораздо удобнее формулы (9.1), в чем вы 
  сможете убедиться в дальнейшем.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Предположим, 
  что у пациента имеется некоторый симптом заболевания, например боль в груди, 
  и желательно знать, какова вероятность того, что этот симптом является следствием 
  определенного заболевания, например инфаркта миокарда или перикардита (воспаление 
  каверн в легких), или чего-нибудь менее серьезного, вроде несварения желудка. 
  Для того чтобы вычислить вероятность <i>Р(инфаркт миокарда боль в груди) </i>по 
  формуле (9.1), нужно знать (или оценить каким-либо способом), сколько человек 
  в мире страдают таким заболеванием и сколько человек <i>и </i>больны инфарктом 
  миокарда, <i>и </i>жалуются на боль в груди (т.е. имеют такой же симптом). Как 
  правило, такая информация отсутствует, особенно последняя, которая нужна для 
  вычисления вероятности <i>Р (инфаркт миокарда </i>л <i>боль в груди). </i>Таким 
  образом, определение, данное формулой (9.1), в клинической практике не может 
  быть использовано.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Отмеченная 
  сложность получения нужной информации явилась причиной негативного отношения 
  многих специалистов по искусственному интеллекту к вероятностному подходу вообще 
  (см., например, <i>[Charniak and McDermott, 1985, Chapter 8]). </i>Это негативное 
  отношение подкреплялось тем, что в большинстве классических работ по теории 
  вероятностей понятие вероятности определялось как <i>объективная частотность 
  </i>(частота появления при достаточно продолжительных независимых испытаниях).<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Однако существует 
  мнение, что эти базовые предположения небесспорны с точки зрения практических 
  приложений (см., например, <i>[Pearl, 1982] </i>и <i>[Cheeseman, 1985]). </i>Сторонники 
  такого подхода придерживаются <i>субъективистской </i>точки зрения на определение 
  вероятности, который позволяет иметь дело с оценками совместного появления событий, 
  а не с действительной частотой. Такой взгляд на вещи связывает вероятность смеси 
  событий с субъективной верой в то, что событие действительно наступит.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Например, 
  врач может не знать или не иметь возможности вычислить, какая часть пациентов, 
  жалующихся на боль в груди, страдает инфарктом миокарда, но на основании собственного 
  опыта он может оценить, у какой части его пациентов, страдающих этим заболеванием, 
  встречался такой симптом. Следовательно, он может оценить значение вероятности 
  <i>Р(боль в груди | инфаркт миокарда). </i>Субъективный взгляд на природу вероятности 
  тесно связан с правилом Байеса по следующей причине. Предположим, мы располагаем 
  достаточно достоверной оценкой вероятности <i>P(s | а), </i>где 5 означает симптом, 
  <i>a d</i>— заболевание. Тогда по формуле (9.2) можно вычислить вероятность 
  <i>P(d\ s). </i>Оценку вероятности <i>P(d) </i>можно взять из публикуемой медицинской 
  статистики, а оценить значение <i>P(s) </i>врач может на основании собственных 
  наблюдений.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Вычисление 
  <i>P(d | s) </i>не вызывает затруднений, когда речь идет о единственном симптоме, 
  т.е. имеется множество заболеваний <i>D </i>и множество симптомов <i>S, </i>причем 
  для каждого члена из <i>D </i>нужно вычислить условную вероятность того, что 
  у пациентов, страдающих этим заболеванием, наблюдался <i>один определенный </i>симптом 
  из множества <i>S. </i>Тем не менее, если в множестве <i>D </i>имеется <i>т 
  </i>членов, а в множестве <i>S</i>— <i>п </i>членов, потребуется вычислить <i>тп 
  + т + п </i>оценок вероятностей. Это отнюдь не простая работа, еcли в системе 
  медицинской диагностики используется до 2000 видов заболеваний и огромное число 
  самых разнообразных симптомов.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Но ситуация 
  значительно усложняется, если мы попробуем включить в процесс составления диагноза 
  не один симптом, а несколько.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В более общей 
  форме правило Байеса имеет вид<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(d|s1^</font></i><font face="Verdana, Arial, Helvetica, sans-serif">...^sk 
  )<b>= </b><i>P(s1^...^s<sub>k</sub>|d)P(d)/P(s1^...^sk)</i></font> <b>(9.3)</b><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">и требует 
  вычисления (mn)<sup>k</sup> + m + <i>n<sup>k</sup> </i>оценок вероятностей, 
  что даже при небольшом значении А; очень много. Эти оценки вероятностей требуются 
  нам по той причине, что в общем случае для вычисления <i>P(s1 ^</i> ....^ s<sub>k</sub>) 
  нужно предварительно вычислить произведения вида<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">P(s<sub>1</sub> 
  | s<sub>2</sub> ^.. <i>.^s<sub>k</sub> </i>)<i>P(s<sub>2</sub> </i>| s<sub>3</sub> 
  ^.. .^s<sub>K</sub> )... <i>P(s<sub>k</sub> </i>) .</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Однако, если 
  предположить, что некоторые симптомы независимы друг от друга, объем вычислений 
  существенно снижается. Независимость любой пары симптомов Si, и <i>Sj </i>означает, 
  что<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(S<sub>i</sub>)=P(S<sub>l</sub>|S<sub>j</sub>),</font></i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">из чего следует 
  соотношение<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(S<sub>i</sub>^S<sub>j</sub>)=P(Si)P(S<sub>j</sub>).</font></i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Если <i>все 
  </i>симптомы независимы, то объем вычислений будет таким же, как и в случае 
  учета при диагнозе единственного симптома.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Но, даже если 
  это и не так, в большинстве случаев можно предположить наличие <i>условной независимости. 
  </i>Это означает, что пара симптомов <i>s\ </i>и <i>Sj </i>является независимой, 
  поскольку в нашем распоряжении имеются какие-либо дополнительные свидетельства 
  на этот счет или фундаментальные знания <i>Е. </i>Таким образом,<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">P(Si|Sj,E)=P(Si|E).</font></i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Например, 
  если в моем автомобиле нет горючего и не работает освещение, я могу смело сказать, 
  что эти симптомы независимы, поскольку моих познаний в устройстве автомобиля 
  вполне достаточно, чтобы предположить, что между ними нет никакой причинной 
  связи. Но если автомобиль не заводится и не работает освещение, то заявлять, 
  что эти симптомы независимы, нельзя, поскольку они могут быть следствием одной 
  и той же неисправности аккумуляторной батареи. Степень доверия к симптому &quot;не 
  работает освещение&quot; только увеличится, если обнаружится, что к тому же 
  и двигатель не заводится. Необходимость отслеживать такого рода связи в программе 
  и соответственно корректировать степень доверия к симптомам значительно увеличивает 
  объем вычислений в общем случае (см. об этом в работе <i>[Cooper, 1990]).</i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Таким образом, 
  использование теории вероятности ставит перед нами следующие проблемы, которые 
  лучше всего сформулировать в терминах задачи выбора:<br>
  </font></p>
<ul>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> <i>либо </i>априори 
    предполагается, что все данные независимы, и использовать менее трудоемкие 
    методы вычислений, за что придется платить снижением достоверности результатов;<br>
    </font></li>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> <i>либо </i>нужно организовать 
    отслеживание зависимости между используемыми данными, количественно оценить 
    эту зависимость, реализовать оперативное обновление соответствующей нормативной 
    информации, т.е. усложнить вычисления, но получить более достоверные результаты.<br>
    </font></li>
</ul>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В главе 19 
  представлен обзор символических методов отслеживания зависимости между используемыми 
  данными, а в главе 21 описаны некоторые численные методы моделирования зависимости 
  между вероятностями.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В следующем 
  разделе мы рассмотрим альтернативный подход, с помощью которого удается обойти 
  указанные сложности при построении экспертных систем. Здесь же, а также в главе 
  21 будут проанализированы критические замечания, касающиеся этого подхода.</font></p>
<p>&nbsp;</p><table BORDER=0   COLS=3 WIDTH="16%" >
  <tr> 
    <td><font face="Arial, Helvetica, sans-serif"><a href="Index3.htm"><img SRC="Back.gif"  BORDER=0 ></a></font></td>
    <td WIDTH="10%"><font face="Arial, Helvetica, sans-serif"><a href="../index.html"><img SRC="Menu.gif" BORDER=0 ></a></font></td>
    <td ALIGN=RIGHT><font face="Arial, Helvetica, sans-serif"><a href="Index5.htm"><img SRC="For.gif" BORDER=0 ></a></font></td>
  </tr>
</table>
</body>
</html>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<html>
<head>
<title></title>
<meta http-equiv="Content-Type" content="text/html;charset=windows-1251">
</head>
<body TEXT="#000000" BGCOLOR="#E7E3E7" LINK="#004080" VLINK="#004080" olink="#008080" Background="">
<table BORDER =0  COLS=3 WIDTH="16%" >
  <tr> 
    <td><font face="Arial, Helvetica, sans-serif"><a href="Index9.htm"><img SRC="Back.gif"  BORDER=0 ></a></font></td>
    <td WIDTH="10%"><font face="Arial, Helvetica, sans-serif"><a href="../index.html"><img SRC="Menu.gif" BORDER=0 ></a></font></td>
    <td ALIGN=RIGHT><font face="Arial, Helvetica, sans-serif"><a href="Index11.htm"><img SRC="For.gif" BORDER=0 ></a></font></td>
  </tr>
</table>
<p>&nbsp;</p>
<p align="center"><font face="Arial, Helvetica, sans-serif" size="3"><font size="4">20.3.2. 
  Алгоритм формирования дерева решений по обучающей выборке</font><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Ниже будет 
  описан алгоритм формирования дерева решений по обучающей выборке, использованный 
  в системе IDЗ. Задача, которую решает алгоритм, формулируется следующим образом. 
  Задано:<br>
  </font></p>
<ul>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> множество целевых непересекающихся 
    классов <i>{С1, С2, ..., Сk};</i><br>
    </font></li>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> обучающая выборка S, 
    в которой содержатся объекты более чем одного класса.<br>
    </font></li>
</ul>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Алгоритм использует 
  последовательность тестовых процедур, с помощью которых множество 5 разделяется 
  на подмножества, содержащие объекты только одного класса. Ключевой в алгоритме 
  является процедура построения дерева решений, в котором нетерминальные узлы 
  соответствуют тестовым процедурам, каждая из которых имеет дело с единственным 
  атрибутом объектов из обучающей выборки. Как вы увидите ниже, весь фокус состоит 
  в в выборе этих тестов.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Пусть <i>Т 
  </i>представляет любую тестовую процедуру, имеющую дело с одним из атрибутов, 
  а {О1,O2,...,On}<i> </i>— множество допустимых выходных значений такой процедуры<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">при ее применении 
  к произвольному объекту <i>х. </i>Применение процедуры <i>Т </i>к объекту <i>х 
  </i>будем обозначать как <i>Т(х). </i>Следовательно, процедура <i>Т(х) </i>разбивает 
  множество S на составляющие {<i>S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>n</sub>}, 
  </i>такие, что<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">S<sub>i</sub>= 
  {x|T(x) = O<sub>i</sub>}. Такое разделение графически представлено на рис. 20.3.<br>
  </font></p>
<p align="center"><font face="Arial, Helvetica, sans-serif"><img src="3.gif" width="550" height="220"> 
  </font></p>
<p align="center"><font face="Arial, Helvetica, sans-serif" size="3">Рис. 20.3. 
  Дерево разделения объектов обучающей выборки<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Если рекурсивно 
  заменять каждый узел S<sub>i</sub>, на рис. 20.3 поддеревом, то в результате 
  будет построено дерево решений для обучающей выборки <i>S. </i>Как уже отмечалось 
  выше, ключевым фактором в решении этой проблемы является выбор тестовой процедуры 
  — для каждого поддерева нужно найти наиболее подходящий атрибут, по которому 
  можно выполнять дальнейшее разделение объектов.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Квинлан (Quinlan) 
  использует для этого заимствованное из теории информации понятие <i>неопределенности. 
  </i>Неопределенность — это число, описывающее множество сообщений M= { <i>m<sub>1</sub>,</i> 
  <i>т<sub>2</sub>,</i>..., <i>т<sub>n</sub>}. </i>Вероятность получения определенного 
  сообщения <i>m<sub>i</sub> </i>из этого множества определим как <i>р(т<sub>i</sub>). 
  </i>Объем информации, содержащейся в этом сообщении, будет в таком случае равен<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">I(m<sub>i</sub>) 
  = -logp(m<sub>i</sub>).</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Таким образом, 
  объем информации в сообщении связан с вероятностью получения этого сообщения 
  обратной монотонной зависимостью. Поскольку объем информации измеряется в битах, 
  логарифм в этой формуле берется по основанию 2.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Неопределенность 
  множества сообщений <i>U(M) </i>является взвешенной суммой количества информации 
  в каждом отдельном сообщении, причем в качестве весовых коэффициентов используются 
  вероятности получения соответствующих сообщений:<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">U(М) 
  = -Sum<sub>ip</sub>[<i>(m<sub>i</sub>) </i>logp(m<sub>i</sub>), <i>i </i>= 1,..., 
  <i>п.</i>]</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Интуитивно 
  ясно, что чем большую неожиданность представляет получение определенного сообщения 
  из числа возможных, тем более оно информативно. Если все сообщения в множестве 
  равновероятны, энтропия множества сообщений достигает максимума.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Тот способ, 
  который использует Квинлан, базируется на следующих предположениях.<br>
  </font></p>
<ul>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> Корректное дерево решения, 
    сформированное по обучающей выборке <i>S, </i>будет разделять объекты в той 
    же пропорции, в какой они представлены в этой обучающей выборке.<br>
    </font></li>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> Для какого-либо объекта, 
    который нужно классифицировать, тестирующую процедуру можно рассматривать 
    как источник сообщений об этом объекте.<br>
    </font></li>
</ul>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Пусть <i>N<sub>i</sub> 
  </i>— количество объектов в S, принадлежащих классу С<sub>i</sub>. Тогда вероятность 
  того, что произвольный объект с, &quot;выдернутый&quot; из <i>S, </i>принадлежит 
  классу С<sub>i</sub>, можно оценить по формуле<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">p(c~C<sub>i</sub>) 
  = N<sub>i</sub>/|S|,</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">а количество 
  информации, которое несет такое сообщение, равно I<i>(с ~</i> С<sub>i</sub>) 
  = -1оg<sub>2</sub>р(m<sub>i</sub>) (<i>с ~</i> С<sub>i</sub>) бит.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Теперь рассмотрим 
  энтропию множества целевых классов, считая их также множеством сообщений {С<sub>1</sub> 
  <i>C<sub>2</sub>, </i>..., C<sub>k</sub>}. Энтропия также может быть вычислена 
  как взвешенная сумма количества информации в отдельных сообщениях, причем весовые 
  коэффициенты можно определить, опираясь на &quot;представительство&quot; классов 
  в обучающей выборке:<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">U(M) 
  = </font></i><font face="Verdana, Arial, Helvetica, sans-serif">-Sum<sub>i=1...,k</sub>[ 
  <i>р(с ~</i> C<sub>i</sub>)x I(с ~ С<sub>i</sub>)] бит.</font><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Энтропия <i>U(M) 
  </i>соответствует среднему количеству информации, которое необходимо для определения 
  принадлежности произвольного объекта <i>(с ~ S) </i>какому-то классу до того, 
  как выполнена хотя бы одна тестирующая процедура. После того как соответствующая 
  тестирующая процедура <i>Т </i>выполнит разделение <i>S </i>на подмножества 
  {S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>n</sub><i>}, </i>энтропия будет определяться 
  соотношением<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">Uт(S) 
  = -Sum<sub>i=1,...k</sub>(|S|/|S<sub>i</sub>|)х U(S<sub>i</sub>).</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Полученная 
  оценка говорит нам, сколько информации еще необходимо после того, как выполнено 
  разделение. Оценка формируется как сумма неопределенностей сформированных подмножеств, 
  взвешенная в пропорции размеров этих подмножеств.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Из этих рассуждений 
  очевидно следует эвристика выбора очередного атрибута для тестирования, используемая 
  в алгоритме, — нужно выбрать тот атрибут, который сулит наибольший прирост информации. 
  Прирост информации <i>G<sub>S</sub>(T) </i>после выполнения процедуры тестирования 
  <i>Т </i>по отношению к множеству 5 равен<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">G<sub>S</sub>(7)=U(S)-Uт(S).</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Такую эвристику 
  иногда называют &quot;минимизацией энтропии&quot;, поскольку увеличивая прирост 
  информации на каждом последующем тестировании, алгоритм тем самым уменьшает 
  энтропию или меру беспорядка в множестве.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Вернемся теперь 
  к нашему примеру с погодой и посмотрим, как эти формулы интерпретируются в самом 
  простом случае, когда множество целевых классов включает всего два элемента. 
  Пусть <i>р </i>— это количество объектов класса П в множестве обучающей выборки 
  <i>S, а п </i>— количество объектов класса Н в этом же множестве. Таким образом, 
  произвольный объект принадлежит к классу П с вероятностью <i>p / (p + п), </i>а 
  к классу Н с вероятностью <i>n /(p + п). </i>Ожидаемое количество информации 
  в множестве сообщений <i>М = </i>{П, Н} равно<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3"><i>U(M) 
  = -p / (p + п) log<sub>2</sub>(p/(p + n </i>)) - </font></p>
<p align="left"><font face="Verdana, Arial, Helvetica, sans-serif" size="3"><i>n 
  / (p </i>+ n) 1оg<sub>2</sub>(n/(р + <i>п))</i></font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Пусть тестирующая 
  процедура <i>Т, </i>как и ранее, разделяет множество <i>S </i>на подмножества 
  <i>{S<sub>1</sub>, S<sub>2</sub></i>.....S<sub>n</sub><i>}, </i>и предположим, 
  что каждый компонент S, содержит p<sub>i</sub>, объектов класса<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">П и и, объектов 
  класса Н. Тогда энтропия каждого подмножества S<sub>i</sub> будет равна<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3">U(S<sub>i</sub>) 
  = <i>-р<sub>i</sub>/(р<sub>i</sub> + n<sub>i</sub>) l</i>og<sub>2</sub>(p<sub>i</sub>/(p<sub>i</sub> 
  + n<sub>i</sub>)) - </font></p>
<p align="left"><font face="Verdana, Arial, Helvetica, sans-serif" size="3">n/(р<sub>i</sub> 
  + <i>n<sub>i</sub>) l</i>og2(n<sub>i</sub>/(p<sub>i</sub> +n<sub>i</sub>))</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Ожидаемое 
  количество информации в той части дерева, которая включает корневой узел, можно 
  представить в виде взвешенной суммы:<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i>U</i>т(<i>S) 
  </i>= -Sum<sub>i=1,...n</sub>((p<i><sub>i</sub>, + n<sub>i</sub>)/</i>(р + n)) 
  х U(S<sub>i</sub>) Отношение (р, + <i>п,)/(р + п) </i>соответствует весу каждой 
  i-и ветви дерева, вроде того, которое показано на рис. 20.3. Это отношение показывает, 
  какая часть всех объектов S принадлежит подмножеству S,.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Ниже мы покажем, 
  что в последней версии этого алгоритма, использованной в системе <b><i>С4.5, 
  </i></b>Квинлан выбрал слегка отличающуюся эвристику. Использование меры прироста 
  информации в том виде, в котором она определена чуть выше, приводит к тому, 
  что предпочтение отдается тестирующим процедурам, имеющим наибольшее количество 
  выходных значений {O<sub>1</sub> O<sub>2</sub>,..., О<i><sub>п</sub></i>}<i>.</i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Но несмотря 
  на эту &quot;загвоздку&quot;, описанный алгоритм успешно применялся при обработке 
  достаточно внушительных обучающих выборок (см., например, <i>[Quintan, 1983]). 
  </i>Сложность алгоритма зависит в основном от сложности процедуры выбора очередного 
  теста для дальнейшего разделения обучающей выборки на все более мелкие подмножества, 
  а последняя линейно зависит от произведения количества объектов в обучающей 
  выборке на количество атрибутов, использованное для их представления. Кроме 
  того, система может работать с зашумленными и неполными данными, хотя этот вопрос 
  в данной книге мы и не рассматривали (читателей, интересующихся этим вопросом, 
  я отсылаю к работе <i>[Quinlan, 1986, b]).</i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Простота и 
  эффективность описанного алгоритма позволяет рассматривать его в качестве достойного 
  соперника существующим процедурам извлечения знаний у экспертов в тех применениях, 
  где возможно сформировать репрезентативную обучающую выборку. Но, в отличие 
  от методики, использующей пространства версий, такой метод не может быть использован 
  инкрементально, т.е. нельзя &quot;дообучить&quot; систему, представив ей новую 
  обучающую выборку, без повторения обработки ранее просмотренных выборок.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Рассмотренный 
  алгоритм также не гарантирует, что сформированное дерево решений будет самым 
  простым из возможных, поскольку использованная оценочная функция, базирующаяся 
  на выводах теории информации, является только эвристикой. Но многочисленные 
  эксперименты, проведенные с этим алгоритмом, показали, что формируемые им деревья 
  решений довольно просты и позволяют прекрасно справиться с классификацией объектов, 
  ранее неизвестных системе. Продолжение поиска &quot;наилучшего решения&quot; 
  приведет к усложнению алгоритма, а как уже отмечалось в главе 14, для многих 
  сложных проблем вполне достаточно найти не лучшее, а удовлетворительное решение.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В состав программного 
  комплекса <i>С4.5, </i>в котором используется описанный выше алгоритм, включен 
  модуль <b><i>C4.5Rules, </i></b>формирующий из дерева решений набор порождающих 
  правил. В этом модуле применяется <i>эвристика отсечения, с </i>помощью которой 
  дерево решений упрощается. При этом, во-первых, формируемые правила становятся 
  более понятными, а значит, упрощается сопровождение экспертной системы, а во-вторых, 
  они меньше зависят от использованной обучающей выборки. Как уже упоминалось, 
  в С4.5 также несколько модифицирован критерий отбора тестирующих процедур по 
  сравнению с оригинальным алгоритмом, использованным в ID3.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Недостатком 
  эвристики, основанной на приросте количества информации, является то, что она 
  отдает предпочтение процедурам с наибольшим количеством выходных значений <i>{О<sub>1</sub>, 
  O<sub>2</sub>, ..., О<sub>n</sub>}. </i>Возьмем, например, крайний случай, когда 
  практически бесполезные тесты будут разделять исходную обучающую выборку на 
  множество классов с единственным представителем в каждом. Это произойдет, если 
  обучающую выборку с медицинскими данными пациентов классифицировать по именам 
  пациентов. Для описанной эвристики именно такой вариант получит преимущество 
  перед прочими, поскольку <i>U<sub>T</sub>(S) </i>будет равно нулю и, следовательно, 
  разность <i>G<sub>s</sub>(T) </i>= <i>U(S) - U<sub>T</sub>(S) </i>достигнет 
  максимального значения.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Для заданной 
  тестирующей процедуры <i>Т </i>на множестве данных <i>S, </i>которая характеризуется 
  приростом количества информации G<sub>S</sub>{T), мы теперь возьмем в качестве 
  критерия отбора <i>относительный прирост Н<sub>S</sub>(Т), </i>который определяется 
  соотношением<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3"><i><font face="Verdana, Arial, Helvetica, sans-serif">Н<sub>S</sub>{Т) 
  = G<sub>S</sub>(Т)/V(S), </font></i></font></p>
<p align="left"><font face="Verdana, Arial, Helvetica, sans-serif" size="3">где<br>
  </font></p>
<p align="left"> <font face="Verdana, Arial, Helvetica, sans-serif" size="3"><i>V(S) 
  </i>= -Sum<sub>i=1</sub>,..., (|S|/|S<sub>i</sub>|) x log2(|S|/|S<sub>i</sub>|).</font><font face="Arial, Helvetica, sans-serif" size="3"><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Важно разобраться, 
  в чем состоит отличие величины <i>V(S) </i>от <i>U(S). </i>Величина <i>V(S) 
  </i>определяется множеством сообщений <i>{О<sub>1</sub>, О<sub>2</sub>,...,О<sub>n</sub>] 
  </i>или, что то же самое, множеством подмножеств {<i>S<sub>1</sub> S<sub>2</sub>,...,S<sub>n</sub>}, 
  </i>ассоциированных с выходными значениями тестовой процедуры, а не с множеством 
  классов {С<sub>1</sub> C<sub>2</sub>,...,C<sub>k</sub>}. Таким образом, при 
  вычислении величины V(S) принимается во внимание множество выходных значений 
  теста, а не множество классов.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Новая эвристика 
  состоит в том, что выбирается та тестирующая процедура, которая максимизирует 
  определенную выше величину относительного прироста количества информации. Теперь 
  те пустые тесты, о которых мы говорили чуть выше и которым прежний алгоритм 
  отдал бы преимущество, окажутся в самом &quot;хвосте&quot;, поскольку для них 
  знаменатель будет равен log<sub>2</sub>(N), где <i>N</i>— количество элементов 
  в обучающей выборке.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Оригинальный 
  алгоритм формирования дерева страдает еще одной &quot;хворью&quot; - он часто 
  формирует сложное дерево, в котором фиксируются несущественные для задачи классификации 
  отличия в элементах обучающей выборки. Один из способов справиться с этой проблемой 
  — использовать правило &quot;останова&quot;, которое прекращало бы процесс дальнейшего 
  разделения ветвей дерева при выполнении определенного условия. Но оказалось, 
  что сформулировать это условие не менее сложно, а потому Квинлан пошел по другому 
  пути. Он решил &quot;обрезать&quot; дерево решений после того, как оно будет 
  сформировано алгоритмом. Можно показать, что такое &quot;обрезание&quot; может 
  привести к тому, что новое дерево будет обрабатывать обучающую выборку с ошибками, 
  но с новыми данными оно обычно справляется лучше, чем полное дерево. Проблема 
  &quot;обрезания&quot; довольно сложна и выходит за рамки данной книги. Читателям, 
  которые заинтересуются ею, я рекомендую познакомиться с работами <i>[Mingers, 
  1989, b] </i>и <i>[Mitchell, 1997], </i>а подробное описание реализации этого 
  процесса в С4.5 можно найти в <i>[Quinlan, 1993, Chapter 4].</i><br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Для того чтобы 
  сделать более понятным результат выполнения алгоритма, в системе <b><i>С4.5 
  </i></b>дерево решений преобразуется в набор порождающих правил. Мы уже ранее 
  демонстрировали соответствие между отдельным путем на графе решений от корня 
  к листу и порождающим правилом. Условия в правиле — это просто тестовые процедуры, 
  выполняемые в промежуточных узлах дерева, а заключение правила — отнесение объекта 
  к определенному классу.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Однако строить 
  набор правил перечислением всех возможных путей на графе — процесс весьма неэффективный. 
  Некоторые тесты могут служить просто для того, чтобы разделить дерево и таким 
  образом сузить пространство выбора до подмножества, которое в дальнейшем уточняется 
  с помощью проверки других, более информативных атрибутов. Это происходит по 
  той причине, что не все атрибуты имеют отношение ко всем классам объектов.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Квинлан применил 
  следующую стратегию формирования множества правил из дерева решений.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">(1) Сформировать 
  начальный вариант множества правил, перечислив все пути от корня дерева к листьям.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">(2) Обобщить 
  правила и при этом удалить из них те условия, которые представляются излишними.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">(3) Сгруппировать 
  правила в подмножества в соответствии с тем, к каким классам они имеют отношение, 
  а затем удалить из каждого подмножества те правила, которые не вносят ничего 
  нового в определение соответствующего класса.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">(4) Упорядочить 
  множества правил по классам и выбрать класс, который будет являться классом 
  по умолчанию.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Упорядочение 
  правил, которое выполняется на шаге (4), можно рассматривать как примитивную 
  форму механизма разрешения конфликтов (см. главу 5). Порядок классов внутри 
  определенного подмножества теперь уже не будет иметь значения. Назначение класса 
  по умолчанию можно считать своего рода правилом по умолчанию, которое действует 
  в том случае, когда не подходит ни одно другое правило.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Полученное 
  в результате множество правил скорее всего не будет точно соответствовать исходному 
  дереву решений, но разобраться в них будет значительно проще, чем в логике дерева 
  решений. При необходимости эти правила можно будет затем уточнить вручную.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">Квинлан очень 
  осторожно подошел к формулировке тех условий, при которых созданная им система 
  <b><i>С4.5 </i></b>может быть использована как подходящий инструмент обучения, 
  позволяющий ожидать удовлетворительных результатов. Подход, основанный на использовании 
  дерева решений, можно применять для решения далеко не всех задач классификации. 
  Определенные ограничения свойственны и тем конкретным алгоритмам, которые использованы 
  в системе <b><i>С4.5. </i></b>Необходимым условием успешного применения этой 
  системы является выполнение следующих требований.<br>
  </font></p>
<ul>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> Перечень классов, с 
    которыми в дальнейшем будет оперировать экспертная система, необходимо сформулировать 
    заранее. Другими словами, алгоритмы, положенные в основу функционирования 
    системы <b><i>С4.5, </i></b>не способны формировать перечень классов на основе 
    группировки обучающей последовательности объектов. Кроме того, классы должны 
    быть четко очерченными, а не &quot;расплывчатыми&quot; — некоторый объект 
    либо принадлежит к данному классу, либо нет, никаких промежуточных состояний 
    быть не может. И, кроме того, классы не должны перекрываться.<br>
    </font></li>
  <li> <font face="Arial, Helvetica, sans-serif" size="3"> Применяемые в системе 
    методы обучения требуют использовать обучающие выборки большого объема. Чем 
    больше объем выборки, тем лучше. При малой длине обучающей выборки на полученных 
    в результате правилах будут сказываться индивидуальные особенности экземпляров 
    в обучающей выборке, что может привести к неверной классификации незнакомых 
    объектов. Методы &quot;усечения&quot; дерева решений, использованные в <i>С4.5, 
    </i>будут работать некорректно, если длина обучающей выборки слишком мала 
    и содержит нетипичные объекты классов.<br>
    </font></li>
</ul>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">4 Данные в 
  обучающей выборке должны быть представлены в формате &quot;атрибут-значение&quot;, 
  т.е. каждый объект должен быть охарактеризован в терминах фиксированного набора 
  атрибутов и их значений для данного объекта. Существуют методы обработки, которые 
  позволяют справиться и с пропущенными атрибутами, — предполагается, что в таких 
  случаях выход соответствующей тестирующей процедуры будет в вероятностном смысле 
  распределен по закону, определенному на основе тех объектов, в которых такой 
  атрибут определен.<br>
  </font></p>
<p align="left"> <font face="Arial, Helvetica, sans-serif" size="3">В тех областях 
  применения, в которых можно использовать и подход, базирующийся на дереве решений, 
  и обычные статистические методы, выбор первого дает определенные преимущества. 
  Этот подход не требует знания никаких априорных статистических характеристик 
  классифицируемого множества объектов, в частности функций распределения значений 
  отдельных атрибутов (использование статистических методов зачастую основано 
  на предположении о существовании нормального распределения значений атрибутов). 
  Как показали эксперименты с экспертными системами классификации разных типов, 
  те из них, в которых используются деревья решений, выигрывают по сравнению с 
  другими по таким показателям, как точность классификации, устойчивость к возмущениям 
  и скорость вычислений.<br>
  </font></p>
<p>&nbsp;</p><table BORDER =0  COLS=3 WIDTH="16%" >
  <tr> 
    <td><font face="Arial, Helvetica, sans-serif"><a href="Index9.htm"><img SRC="Back.gif"  BORDER=0 ></a></font></td>
    <td WIDTH="10%"><font face="Arial, Helvetica, sans-serif"><a href="../index.html"><img SRC="Menu.gif" BORDER=0 ></a></font></td>
    <td ALIGN=RIGHT><font face="Arial, Helvetica, sans-serif"><a href="Index11.htm"><img SRC="For.gif" BORDER=0 ></a></font></td>
  </tr>
</table>
</body>
</html>